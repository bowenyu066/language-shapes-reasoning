## Introduction

Sequence length is a critical bottleneck in modern deep learning. Across domains—from autoregressive language models to vision transformers to protein structure predictors—the Transformer architecture has become the dominant paradigm, yet its core self-attention mechanism scales quadratically with sequence length, $O(n^2 d)$. This computational constraint forces practitioners into a fundamental trade-off: longer sequences enable richer representations and more complex reasoning, but at rapidly escalating cost in memory and compute.

This tension has motivated an extensive body of research aimed at a single objective: *processing representations more efficiently*. Sparse attention patterns reduce the number of pairwise interactions. Learned compression tokens distill long contexts into fixed-size summaries. Linear attention approximations replace quadratic operations with more tractable alternatives. In vision, patch pruning and latent space compression serve analogous goals. These methods share a common assumption—that the input representation is fixed, and efficiency must be achieved through algorithmic innovation in how we process it.

Yet this framing neglects a more fundamental degree of freedom: *the representation itself*. The same underlying information can be encoded in vastly different ways, each yielding different sequence lengths. An image can be tokenized into 256 patches or 1024. A protein sequence can be represented at the residue level or compressed into structural motifs. A paragraph of text might occupy 100 tokens in one encoding scheme and 60 in another. The choice of representation—not merely the method of processing—determines the token budget required for a given task.

This observation raises a central question for deep learning systems: **Does task performance depend on the representation, or only on the information content it encodes?** If a model can reason equally well over a 60-token representation as a 100-token one carrying the same information, then representation design becomes a lever for efficiency gains that compound with scale. Conversely, if compressed representations degrade performance, then the efficiency-quality trade-off must be carefully navigated.

This trade-off has been studied extensively in some domains. In computer vision, aggressive compression of latent spaces degrades reconstruction quality in predictable ways, and the rate-distortion curve is well characterized. In learned image compression, the relationship between bitrate and perceptual quality follows established information-theoretic principles. However, in the context of *reasoning*—where the goal is not reconstruction but inference over semantic content—this trade-off remains poorly understood. Can a model solve a math problem equally well when the problem statement is encoded in fewer tokens? Does the density of a representation impose limits on the complexity of reasoning it can support?

To investigate these questions empirically, we require a setting where (1) the same semantic content can be naturally expressed in representations of different lengths, (2) task performance can be precisely measured, and (3) we can isolate representation effects from confounding factors. Natural language provides an ideal experimental substrate. Human languages encode equivalent semantic content with dramatically different token efficiencies. Consider the sentence: "The quick brown fox jumps over the lazy dog." In English, a typical tokenizer produces roughly 10 tokens. The Chinese translation ("快速的棕色狐狸跳过懒狗") compresses to 4–6 tokens while preserving semantic content. This variation is not artificial—it reflects genuine differences in how information is encoded across writing systems and linguistic structures.

Crucially, we frame this not as a study of multilingualism per se, but as an investigation into **representation efficiency and its effect on model reasoning**. Languages serve as naturally occurring, semantically-aligned representations of varying density. By evaluating models on equivalent mathematical problems expressed in different languages, we can measure whether denser representations (fewer tokens) maintain reasoning quality—or whether they expose architectural bottlenecks that degrade performance.

Our experimental design targets three progressively refined questions:

1. **Do well-trained models exhibit representation-invariant reasoning?** We evaluate state-of-the-art models on multilingual mathematical benchmarks to test whether accuracy depends on the language of presentation—i.e., the representation—or only on the underlying mathematical content.

2. **What happens when models are less comprehensively trained?** By comparing models with different training distributions (multilingual vs. English-centric), we can identify whether performance gaps reflect fundamental architectural limitations or merely training data coverage.

3. **Are representation bottlenecks learnable?** If a model struggles with a particular representation, can targeted fine-tuning recover performance? This tests whether the bottleneck is intrinsic to the architecture or an artifact of training.

Our findings reveal a consistent pattern: representation choice affects token efficiency but need not degrade reasoning quality—provided the model has been adequately trained on the target representation. State-of-the-art models achieve near-identical accuracy across languages while using 5–10% fewer tokens in denser representations like Chinese. Models with narrower training distributions exhibit significant performance gaps, but these gaps can be substantially reduced through modest fine-tuning. Together, these results suggest that representation efficiency is not a trade-off to accept, but a capability to unlock through appropriate training.

The implications extend beyond multilingual modeling. If reasoning quality is invariant to representation density for well-trained models, then representation design becomes a first-class optimization target. Rather than engineering ever-more-efficient attention mechanisms to process fixed-length inputs, we might instead invest in representations that encode the same information in fewer tokens. Natural languages offer one such family of representations; learned compression schemes, domain-specific tokenizations, and structured encodings offer others. Our work provides empirical grounding for this perspective and a methodology for evaluating representation efficiency in reasoning tasks.
