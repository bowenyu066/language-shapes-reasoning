## Introduction

Scaling sequence length has emerged as a dominant strategy for improving model performance. From extended context windows in Large Language Models to higher-resolution feature maps in vision transformers, the underlying premise is consistent: more tokens enable richer representations and more nuanced reasoning. Yet this pursuit collides with a fundamental constraint—the quadratic cost of self-attention, $O(n^2 d)$, which causes computational and memory requirements to escalate rapidly as sequence length $(n)$ grows.

This tension has spawned a rich body of work focused on a single question: *how can we process the same representation more efficiently?* In language modeling, researchers have proposed sparse attention patterns, learned compression tokens, and efficient approximations to reduce effective sequence length during inference. In computer vision, analogous efforts seek to lower the rank of latent spaces or prune redundant patches. These methods share a common assumption: the input representation is fixed, and efficiency gains must come from smarter processing.

However, this framing overlooks a more fundamental degree of freedom: *the same information can be encoded in drastically different ways*. In language, identical semantic content can be expressed across languages with vastly different token counts. In vision, autoencoders can learn compressed latent codes of varying dimensionality. In computational biology, the same protein can be embedded using different learned representations. The choice of representation—not just the method of processing—determines the token budget required.

This distinction matters because different representations exhibit markedly different trade-offs between token efficiency and task performance. While this trade-off has been extensively studied in domains like vision (where reconstruction quality degrades with aggressive compression), it remains underexplored in language, where tokenization is often treated as a preprocessing detail rather than a design variable.

To investigate this trade-off, we use multilingual Large Language Models as a strategic entry point. Consider the sentence: "The quick brown fox jumps over the lazy dog." In English, this consumes roughly 10 tokens. In Chinese ("快速的棕色狐狸跳过懒狗"), the same semantic content compresses to just 4–6 tokens. This natural variation provides an ideal experimental proxy: rather than engineering artificial compression schemes, we can exploit the inherent density differences across languages to test whether reduced sequence lengths offer genuine computational advantages—or whether high-density tokenization exposes architectural bottlenecks that degrade reasoning capability.